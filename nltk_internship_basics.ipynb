{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIM51SYd72jD+LZeTpw2ve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saun09/Internship-1/blob/main/nltk_internship_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raZ9P_n8reg-",
        "outputId": "02ecd715-6d48-4642-eca2-824d26eb1924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib\",\n",
              " 'learned',\n",
              " 'rapidly',\n",
              " 'because',\n",
              " 'his',\n",
              " 'first',\n",
              " 'training',\n",
              " 'was',\n",
              " 'in',\n",
              " 'how',\n",
              " 'to',\n",
              " 'learn',\n",
              " '.',\n",
              " '``',\n",
              " ',',\n",
              " \"'And\",\n",
              " 'the',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'of',\n",
              " 'all',\n",
              " 'was',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'trust',\n",
              " 'that',\n",
              " 'he',\n",
              " 'could',\n",
              " 'learn',\n",
              " '.',\n",
              " \"'\",\n",
              " ',',\n",
              " \"''\",\n",
              " 'It',\n",
              " \"'s\",\n",
              " 'shocking',\n",
              " 'to',\n",
              " 'find',\n",
              " 'how',\n",
              " 'many',\n",
              " 'people',\n",
              " 'do',\n",
              " 'not',\n",
              " 'believe',\n",
              " 'they',\n",
              " 'can',\n",
              " 'learn',\n",
              " ',',\n",
              " 'and',\n",
              " 'how',\n",
              " 'many',\n",
              " 'more',\n",
              " 'believe',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'be',\n",
              " 'difficult',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#tokenize by word (to idenitfy words that come up often) and tokenize by sentence(to understand how words relate and understand the context)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "example_string= \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
        "'And the first lesson of all was the basic trust that he could learn.',\n",
        "\"It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"\"\"\n",
        "sent_tokenize(example_string)\n",
        "word_tokenize(example_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering stop words like in , an , as , is\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stopword_sample= \"\"\" And you shall be; fondly, but you won't care about it because when you're dead you're dead, and until then,there's ice cream.\"\"\"\n",
        "words_to_be_filtered=word_tokenize(stopword_sample) # split quote into words and store in words to be filtered\n",
        "print(\"split words from quote\", words_to_be_filtered)\n",
        "stop_words=set(stopwords.words(\"english\")) #create a set to get all stopwords in english\n",
        "#filtered_words=[] # words that are not stopwords will go here\n",
        "#for word in words_to_be_filtered:\n",
        "#  if word.casefold() not in stop_words: #use casefold() to avoid upper and lower case problem\n",
        "#    filtered_words.append(word) # append words that are not stop words to filtered_words\n",
        "#print(filtered_words)\n",
        "filtered_words=[word for word in words_to_be_filtered if word.casefold() not in stop_words]\n",
        "print(\"not stop words: \" ,filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA0vtk1junfp",
        "outputId": "e3e8f650-02f8-4117-e240-a137731ba40e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split words from quote ['And', 'you', 'shall', 'be', ';', 'fondly', ',', 'but', 'you', 'wo', \"n't\", 'care', 'about', 'it', 'because', 'when', 'you', \"'re\", 'dead', 'you', \"'re\", 'dead', ',', 'and', 'until', 'then', ',', 'there', \"'s\", 'ice', 'cream', '.']\n",
            "not stop words:  ['shall', ';', 'fondly', ',', 'wo', \"n't\", 'care', \"'re\", 'dead', \"'re\", 'dead', ',', ',', \"'s\", 'ice', 'cream', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming- finding root of words to understand its basic meaning\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "string_to_stem=\"\"\"Lay off the anti-depressants. They're making your slow and you're still not happy.\"\"\"\n",
        "#tokenize the sentence into words to stem\n",
        "words=word_tokenize(string_to_stem)\n",
        "stemmed_words=[stemmer.stem(word) for word in words]\n",
        "print(\"stemmed words are: \", stemmed_words)\n",
        "#Snowball stemmer or porter2 is the newer version - more accurate - less under and over stemming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAhFSHdGxWQE",
        "outputId": "1392663a-4e73-449d-8663-d90ebb6d294f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stemmed words are:  ['lay', 'off', 'the', 'anti-depress', '.', 'they', \"'re\", 'make', 'your', 'slow', 'and', 'you', \"'re\", 'still', 'not', 'happi', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos tagging- to check part of speech of word\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "string_to_postag=\"\"\"The people who built this jail were nice enough to put doors in the walls. Why don't they just use them?\"\"\"\n",
        "words=word_tokenize(string_to_postag)\n",
        "tagged_words=nltk.pos_tag(words)\n",
        "print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9Y9L9W1zKYZ",
        "outputId": "9a93813a-7dc0-465d-a822-66d538d27643"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('people', 'NNS'), ('who', 'WP'), ('built', 'VBP'), ('this', 'DT'), ('jail', 'NN'), ('were', 'VBD'), ('nice', 'JJ'), ('enough', 'RB'), ('to', 'TO'), ('put', 'VB'), ('doors', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('walls', 'NNS'), ('.', '.'), ('Why', 'WRB'), ('do', 'VBP'), (\"n't\", 'RB'), ('they', 'PRP'), ('just', 'RB'), ('use', 'VB'), ('them', 'PRP'), ('?', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatizing- meaning of root word but inlike stemming it gives a word that makes sense on its own\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "string_to_lemmatize=\"\"\"My memory is a mighty fortress, Lisbon, from which no fact ever escapes once committed. Now when you tell me boring things, I set them free immediately. It saves overcrowding\"\"\"\n",
        "words=word_tokenize(string_to_lemmatize)\n",
        "lemmatized_words=[lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oz-t74D0yz3",
        "outputId": "ecdec158-91f0-4b22-fb30-869329e69917"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'memory', 'is', 'a', 'mighty', 'fortress', ',', 'Lisbon', ',', 'from', 'which', 'no', 'fact', 'ever', 'escape', 'once', 'committed', '.', 'Now', 'when', 'you', 'tell', 'me', 'boring', 'thing', ',', 'I', 'set', 'them', 'free', 'immediately', '.', 'It', 'save', 'overcrowding']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking- to idenitfy phrases\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "string_to_chunk=\"\"\"If you're going to jump, make sure to dive. Head first.\"\"\"\n",
        "words=word_tokenize(string_to_chunk)\n",
        "tagged_words=nltk.pos_tag(words)\n",
        "print(tagged_words)\n",
        "#create a chunk grammar i.e. rules to follow\n",
        "grammar=\"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunk_parser=nltk.RegexpParser(grammar)\n",
        "tree=chunk_parser.parse(tagged_words)\n",
        "print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i3MCYCC2pt3",
        "outputId": "9f5dc3e1-1cba-490f-eadd-907846a0b85c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('If', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('jump', 'VB'), (',', ','), ('make', 'VB'), ('sure', 'JJ'), ('to', 'TO'), ('dive', 'VB'), ('.', '.'), ('Head', 'NNP'), ('first', 'RB'), ('.', '.')]\n",
            "(S\n",
            "  If/IN\n",
            "  you/PRP\n",
            "  're/VBP\n",
            "  going/VBG\n",
            "  to/TO\n",
            "  jump/VB\n",
            "  ,/,\n",
            "  make/VB\n",
            "  sure/JJ\n",
            "  to/TO\n",
            "  dive/VB\n",
            "  ./.\n",
            "  Head/NNP\n",
            "  first/RB\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}